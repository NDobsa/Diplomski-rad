{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3008abd3",
      "metadata": {
        "id": "3008abd3"
      },
      "source": [
        "**Nina Dobša, zadnje mijenjano 28.7.2025.**\n",
        "\n",
        "> Dodaj oznaku dugog citata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e15e66",
      "metadata": {
        "id": "09e15e66"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "J6yV_qZlJRbx",
        "outputId": "166dab0b-814a-4528-ed4a-d2e0be66a09c"
      },
      "id": "J6yV_qZlJRbx",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.0\n",
            "    Uninstalling scipy-1.16.0:\n",
            "      Successfully uninstalled scipy-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e1c2a81e0ab64da89916d353254b1b47"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7fa1ad7b",
      "metadata": {
        "id": "7fa1ad7b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting with google drive where fine tuned models are stored\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKXMzjRALI-C",
        "outputId": "ed06c226-d76e-4001-b2ef-4cd2115da3ca"
      },
      "id": "HKXMzjRALI-C",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b1f1fc99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1f1fc99",
        "outputId": "c9e0b88a-4be7-4240-9a98-4d5b3bd84f0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7505, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Uploading ISEAR data\n",
        "file_path = \"/content/drive/My Drive/data/isear_3.txt\"\n",
        "ISEAR_data = pd.read_csv(file_path, delimiter=\"|\", header=0, on_bad_lines=\"skip\", engine=\"python\")\n",
        "\n",
        "# Select only Emotion and Text columns from original dataset\n",
        "ISEAR_data = ISEAR_data[['SIT', 'Field1']]\n",
        "ISEAR_data.columns = ['text', 'emotion']\n",
        "ISEAR_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "aa15e2c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa15e2c1",
        "outputId": "29e15d93-dfb1-49f4-eb9a-6b22cc3a2f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n"
          ]
        }
      ],
      "source": [
        "# Encoding emotions (from words to numbers)\n",
        "label_encoder = LabelEncoder()\n",
        "ISEAR_data['emotion_label'] = label_encoder.fit_transform(ISEAR_data['emotion'])\n",
        "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3eabb55f",
      "metadata": {
        "id": "3eabb55f"
      },
      "outputs": [],
      "source": [
        "# Defining number of classes and length of the sequence (if the sentence is shorter padding will be added)\n",
        "num_classes = 7\n",
        "max_sequence_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bc2a375d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "bc2a375d",
        "outputId": "e554d1a0-c271-447e-8f65-62ef0af1dcc6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualization of class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='emotion_label', data=ISEAR_data)\n",
        "plt.title('Distribucija klasa u ISEAR skupu podataka')\n",
        "plt.xlabel('Klasa')\n",
        "plt.ylabel('Broj uzoraka')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1785f5",
      "metadata": {
        "id": "9a1785f5"
      },
      "source": [
        "Classes are balanced..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f5c618",
      "metadata": {
        "id": "09f5c618"
      },
      "source": [
        "# Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5c9d0b62",
      "metadata": {
        "id": "5c9d0b62"
      },
      "outputs": [],
      "source": [
        "# Import of fine tuned word2vec model\n",
        "model_path_SG = \"/content/drive/My Drive/fine_tuned_word2vec_sg/fine_tuned_word2vec_sg.model\"\n",
        "model_path_CBOW = \"/content/drive/My Drive/fine_tuned_word2vec_cbow/fine_tuned_word2vec_cbow.model\"\n",
        "word2vec_model_SG = Word2Vec.load(model_path_SG)\n",
        "word2vec_model_CBOW = Word2Vec.load(model_path_CBOW)\n",
        "\n",
        "embedding_dim_SG = word2vec_model_SG.vector_size\n",
        "embedding_dim_CBOW = word2vec_model_CBOW.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "64a86909",
      "metadata": {
        "id": "64a86909"
      },
      "outputs": [],
      "source": [
        "# Function for getting word2vec embeddings\n",
        "def get_word2vec_embeddings(sentence, model):\n",
        "    # Tokenization\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    word_embeddings = [model.wv[word] for word in words if word in model.wv]\n",
        "\n",
        "    if not word_embeddings:  # If no words are in the vocabulary, return a zero vector\n",
        "        return []\n",
        "\n",
        "    return word_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8467d1af",
      "metadata": {
        "id": "8467d1af"
      },
      "outputs": [],
      "source": [
        "# Getting word2vec embeddings for each word in the sentence (except stowords)\n",
        "list_of_embeddings_CBOW = [\n",
        "   get_word2vec_embeddings(text, word2vec_model_CBOW)\n",
        "    for text in ISEAR_data['text']\n",
        "]\n",
        "\n",
        "list_of_embeddings_SG = [\n",
        "   get_word2vec_embeddings(text, word2vec_model_SG)\n",
        "    for text in ISEAR_data['text']\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "41ffb79a",
      "metadata": {
        "id": "41ffb79a"
      },
      "outputs": [],
      "source": [
        "# Padding sequences to the same length -> adding zeroes to the sequences shorter than embedding_dim and cutting sequences longer than embedding_dim\n",
        "# Converting to NumPy array\n",
        "embeddings_CBOW = pad_sequences(\n",
        "    list_of_embeddings_CBOW,\n",
        "    maxlen = max_sequence_length,\n",
        "    dtype = 'float32',\n",
        "    padding = 'post',\n",
        "    truncating = 'post'\n",
        ")\n",
        "\n",
        "# print(embeddings_CBOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3fb574db",
      "metadata": {
        "id": "3fb574db"
      },
      "outputs": [],
      "source": [
        "# Padding sequences to the same length -> adding zeroes to the sequences shorter than embedding_dim and cutting sequences longer than embedding_dim\n",
        "# Converting to NumPy array\n",
        "embeddings_SG = pad_sequences(\n",
        "    list_of_embeddings_SG,\n",
        "    maxlen = max_sequence_length,\n",
        "    dtype = 'float32',\n",
        "    padding = 'post',\n",
        "    truncating = 'post'\n",
        ")\n",
        "\n",
        "# print(embeddings_SG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "622b2489",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "622b2489",
        "outputId": "840d902a-a7b7-4c0a-aa14-76e6766ca48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of embeddings_CBOW: (7505, 30, 300)\n",
            "Dimensions of embeddings_SG: (7505, 30, 300)\n",
            "Dimensions of target value: (7505, 7)\n"
          ]
        }
      ],
      "source": [
        "# One-hot encoding of target values\n",
        "y = to_categorical(ISEAR_data['emotion_label'], num_classes=num_classes)\n",
        "y_original_labels = ISEAR_data['emotion_label'].values\n",
        "\n",
        "print(f\"Dimensions of embeddings_CBOW: {embeddings_CBOW.shape}\") # (number_of_examples, sequence_length, embedding_dim)\n",
        "print(f\"Dimensions of embeddings_SG: {embeddings_SG.shape}\") # (number_of_examples, sequence_length, embedding_dim)\n",
        "print(f\"Dimensions of target value: {y.shape}\") # (number_of_examples, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce781f5",
      "metadata": {
        "id": "5ce781f5"
      },
      "source": [
        "# RNN predictions for Word2Vec embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d09564",
      "metadata": {
        "id": "71d09564"
      },
      "source": [
        "### Predictions for SG embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3f94496e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f94496e",
        "outputId": "08da0dc3-a174-4c0b-f98a-03e60fb3eaeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Fold 1/5 ---\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "   Fold 1 - Val Loss: 1.3507, Val Acc: 0.5383, Val Precision-macro: 0.5411, Val Recall-macro: 0.5383, Val F1-macro: 0.5377, Val AUC-macro: 0.8474\n",
            "\n",
            "--- Training Fold 2/5 ---\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "   Fold 2 - Val Loss: 1.3344, Val Acc: 0.5390, Val Precision-macro: 0.5538, Val Recall-macro: 0.5393, Val F1-macro: 0.5391, Val AUC-macro: 0.8466\n",
            "\n",
            "--- Training Fold 3/5 ---\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "   Fold 3 - Val Loss: 1.3778, Val Acc: 0.5250, Val Precision-macro: 0.5513, Val Recall-macro: 0.5251, Val F1-macro: 0.5283, Val AUC-macro: 0.8457\n",
            "\n",
            "--- Training Fold 4/5 ---\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "   Fold 4 - Val Loss: 1.2779, Val Acc: 0.5536, Val Precision-macro: 0.5604, Val Recall-macro: 0.5536, Val F1-macro: 0.5528, Val AUC-macro: 0.8583\n",
            "\n",
            "--- Training Fold 5/5 ---\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "   Fold 5 - Val Loss: 1.2938, Val Acc: 0.5383, Val Precision-macro: 0.5513, Val Recall-macro: 0.5385, Val F1-macro: 0.5382, Val AUC-macro: 0.8549\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "n_splits = 5 # 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "lstm_units = 128\n",
        "dropout_rate = 0.3\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_losses = []\n",
        "fold_f1_macros = []\n",
        "fold_auc_macros = []\n",
        "fold_precision_macros = []\n",
        "fold_recall_macros = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(embeddings_SG, y_original_labels)):\n",
        "    print(f\"\\n--- Training Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    X_train_fold, X_val_fold = embeddings_SG[train_index], embeddings_SG[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    clear_session() # Reseting Keras session to build new model\n",
        "\n",
        "    # Building an LSTM Bidirectional model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(max_sequence_length, embedding_dim_SG)))\n",
        "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=False)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # EarlyStopping Callback - to prevent model of overfitting\n",
        "    early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_accuracy', # We are tracking validation accuracy\n",
        "        patience=5,             # The training stops if the val_accuracy doesn't get better after 5 epochs\n",
        "        restore_best_weights=True,\n",
        "        mode='max',             # Maximizing accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Training the model\n",
        "    history = model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        epochs=50,   # trining in 50 epochs (we assume early stopping call back will end the training before)\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        callbacks=[early_stopping_callback],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluating the model\n",
        "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "    fold_losses.append(loss)\n",
        "    fold_accuracies.append(accuracy)\n",
        "\n",
        "    y_pred_probs_fold = model.predict(X_val_fold, verbose=0)\n",
        "    y_pred_classes_fold = np.argmax(y_pred_probs_fold, axis=1)\n",
        "    y_true_classes_fold = np.argmax(y_val_fold, axis=1)\n",
        "\n",
        "    precision_macro_fold = precision_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "    recall_macro_fold = recall_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "\n",
        "    fold_precision_macros.append(precision_macro_fold)\n",
        "    fold_recall_macros.append(recall_macro_fold)\n",
        "\n",
        "    f1_macro_fold = f1_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "    fold_f1_macros.append(f1_macro_fold)\n",
        "\n",
        "    try:\n",
        "        auc_macro_fold = roc_auc_score(y_true_classes_fold, y_pred_probs_fold, multi_class='ovr', average='macro')\n",
        "        fold_auc_macros.append(auc_macro_fold)\n",
        "    except ValueError as e:\n",
        "        fold_auc_macros.append(np.nan)\n",
        "\n",
        "    # Printing results for the current fold\n",
        "    print(f\"   Fold {fold + 1} - Val Loss: {loss:.4f}, Val Acc: {accuracy:.4f}, Val Precision-macro: {precision_macro_fold:.4f}, Val Recall-macro: {recall_macro_fold:.4f}, Val F1-macro: {f1_macro_fold:.4f}, Val AUC-macro: {fold_auc_macros[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0e0d85ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e0d85ef",
        "outputId": "64e373c5-588f-4211-8272-d9c856740db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Average cross validation results for Word2Vec SG embeddings ---\n",
            "Val Loss: 1.3269 +/- 0.0366\n",
            "Val Accuracy: 0.5388 +/- 0.0091\n",
            "Val Precision-macro: 0.5516 +/- 0.0062\n",
            "Val Recall-macro: 0.5390 +/- 0.0090\n",
            "Val F1-macro: 0.5392 +/- 0.0078\n",
            "Val AUC-macro: 0.8506 +/- 0.0051\n"
          ]
        }
      ],
      "source": [
        "# Calculation of average results across all folds +/ std.dev\n",
        "print(\"\\n--- Average cross validation results for Word2Vec SG embeddings ---\")\n",
        "print(f\"Val Loss: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\")\n",
        "print(f\"Val Accuracy: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Val Precision-macro: {np.mean(fold_precision_macros):.4f} +/- {np.std(fold_precision_macros):.4f}\")\n",
        "print(f\"Val Recall-macro: {np.mean(fold_recall_macros):.4f} +/- {np.std(fold_recall_macros):.4f}\")\n",
        "print(f\"Val F1-macro: {np.mean(fold_f1_macros):.4f} +/- {np.std(fold_f1_macros):.4f}\")\n",
        "print(f\"Val AUC-macro: {np.nanmean(fold_auc_macros):.4f} +/- {np.nanstd(fold_auc_macros):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e020a5a9",
      "metadata": {
        "id": "e020a5a9"
      },
      "source": [
        "### Predictions for CBOW embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3160b1d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3160b1d1",
        "outputId": "69789754-bd47-47d1-8b7b-53966273ecf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Fold 1/5 ---\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "   Fold 1 - Val Loss: 1.4269, Val Acc: 0.5010, Val Precision-macro: 0.5106, Val Recall-macro: 0.5010, Val F1-macro: 0.4982, Val AUC-macro: 0.8251\n",
            "\n",
            "--- Training Fold 2/5 ---\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "   Fold 2 - Val Loss: 1.5808, Val Acc: 0.5310, Val Precision-macro: 0.5381, Val Recall-macro: 0.5311, Val F1-macro: 0.5291, Val AUC-macro: 0.8309\n",
            "\n",
            "--- Training Fold 3/5 ---\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "   Fold 3 - Val Loss: 1.5612, Val Acc: 0.5037, Val Precision-macro: 0.5018, Val Recall-macro: 0.5040, Val F1-macro: 0.5008, Val AUC-macro: 0.8203\n",
            "\n",
            "--- Training Fold 4/5 ---\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "   Fold 4 - Val Loss: 1.3899, Val Acc: 0.5203, Val Precision-macro: 0.5241, Val Recall-macro: 0.5204, Val F1-macro: 0.5203, Val AUC-macro: 0.8386\n",
            "\n",
            "--- Training Fold 5/5 ---\n",
            "Epoch 9: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "   Fold 5 - Val Loss: 1.3959, Val Acc: 0.5117, Val Precision-macro: 0.5179, Val Recall-macro: 0.5119, Val F1-macro: 0.5122, Val AUC-macro: 0.8263\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "n_splits = 5 # 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "lstm_units = 128\n",
        "dropout_rate = 0.3\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_losses = []\n",
        "fold_f1_macros = []\n",
        "fold_auc_macros = []\n",
        "fold_precision_macros = []\n",
        "fold_recall_macros = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(embeddings_CBOW, y_original_labels)):\n",
        "    print(f\"\\n--- Training Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    X_train_fold, X_val_fold = embeddings_CBOW[train_index], embeddings_CBOW[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    clear_session() # Reseting Keras session to build new model\n",
        "\n",
        "    # Building an LSTM Bidirectional model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(max_sequence_length, embedding_dim_CBOW)))\n",
        "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=False)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # EarlyStopping Callback - to prevent model of overfitting\n",
        "    early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_accuracy', # We are tracking validation accuracy\n",
        "        patience=5,             # The training stops if the val_accuracy doesn't get better after 5 epochs\n",
        "        restore_best_weights=True,\n",
        "        mode='max',             # Maximizing accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Training the model\n",
        "    history = model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        epochs=50,   # trining in 50 epochs (we assume early stopping call back will end the training before)\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        callbacks=[early_stopping_callback],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluating the model\n",
        "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "    fold_losses.append(loss)\n",
        "    fold_accuracies.append(accuracy)\n",
        "\n",
        "    y_pred_probs_fold = model.predict(X_val_fold, verbose=0)\n",
        "    y_pred_classes_fold = np.argmax(y_pred_probs_fold, axis=1)\n",
        "    y_true_classes_fold = np.argmax(y_val_fold, axis=1)\n",
        "\n",
        "    precision_macro_fold = precision_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "    recall_macro_fold = recall_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "\n",
        "    fold_precision_macros.append(precision_macro_fold)\n",
        "    fold_recall_macros.append(recall_macro_fold)\n",
        "\n",
        "    f1_macro_fold = f1_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "    fold_f1_macros.append(f1_macro_fold)\n",
        "\n",
        "    try:\n",
        "        auc_macro_fold = roc_auc_score(y_true_classes_fold, y_pred_probs_fold, multi_class='ovr', average='macro')\n",
        "        fold_auc_macros.append(auc_macro_fold)\n",
        "    except ValueError as e:\n",
        "        fold_auc_macros.append(np.nan)\n",
        "\n",
        "    # Printing results for the current fold\n",
        "    print(f\"   Fold {fold + 1} - Val Loss: {loss:.4f}, Val Acc: {accuracy:.4f}, Val Precision-macro: {precision_macro_fold:.4f}, Val Recall-macro: {recall_macro_fold:.4f}, Val F1-macro: {f1_macro_fold:.4f}, Val AUC-macro: {fold_auc_macros[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1b6f0222",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b6f0222",
        "outputId": "6c2277a2-651e-42ac-ee22-4be8dcecfa93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Average cross validation results for Word2Vec CBOW embeddings ---\n",
            "Val Loss: 1.4709 +/- 0.0829\n",
            "Val Accuracy: 0.5135 +/- 0.0110\n",
            "Val Precision-macro: 0.5185 +/- 0.0123\n",
            "Val Recall-macro: 0.5137 +/- 0.0110\n",
            "Val F1-macro: 0.5121 +/- 0.0116\n",
            "Val AUC-macro: 0.8282 +/- 0.0062\n"
          ]
        }
      ],
      "source": [
        "# Calculation of average results across all folds +/ std.dev\n",
        "print(\"\\n--- Average cross validation results for Word2Vec CBOW embeddings ---\")\n",
        "print(f\"Val Loss: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\")\n",
        "print(f\"Val Accuracy: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Val Precision-macro: {np.mean(fold_precision_macros):.4f} +/- {np.std(fold_precision_macros):.4f}\")\n",
        "print(f\"Val Recall-macro: {np.mean(fold_recall_macros):.4f} +/- {np.std(fold_recall_macros):.4f}\")\n",
        "print(f\"Val F1-macro: {np.mean(fold_f1_macros):.4f} +/- {np.std(fold_f1_macros):.4f}\")\n",
        "print(f\"Val AUC-macro: {np.nanmean(fold_auc_macros):.4f} +/- {np.nanstd(fold_auc_macros):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49afe98",
      "metadata": {
        "id": "d49afe98"
      },
      "source": [
        "# BERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c5e6a454",
      "metadata": {
        "id": "c5e6a454"
      },
      "outputs": [],
      "source": [
        "# Import of fine tuned bert model\n",
        "model_path = \"/content/drive/My Drive/fine_tuned_bert\"\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "bert_model = BertForMaskedLM.from_pretrained(model_path)\n",
        "embedding_dim_bert = bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bc265cb3",
      "metadata": {
        "id": "bc265cb3"
      },
      "outputs": [],
      "source": [
        "# Function for getting bert embeddings\n",
        "def get_bert_embeddings(sentence, bert_model, bert_tokenizer, max_sequence_length_for_lstm):\n",
        "\n",
        "    padding_zeroes = np.zeros(embedding_dim_bert, dtype=np.float32) # used to pad sequences shorter than sequence_length (50)\n",
        "\n",
        "    inputs = bert_tokenizer(\n",
        "        sentence,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    bert_model.to(device)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs.get('attention_mask', None),\n",
        "            token_type_ids=inputs.get('token_type_ids', None),\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "    # Get the last hidden state (token embeddings)\n",
        "    token_embeddings_np = outputs.hidden_states[-1].squeeze(0).cpu().numpy()\n",
        "    input_ids = inputs['input_ids'].squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Filter out special tokens and empty strings\n",
        "    filtered_embeddings_list = []\n",
        "    for i, token_id in enumerate(input_ids):\n",
        "        token_str = bert_tokenizer.decode(token_id)\n",
        "\n",
        "        if token_str in bert_tokenizer.all_special_tokens or not token_str.strip():\n",
        "            continue\n",
        "\n",
        "        filtered_embeddings_list.append(token_embeddings_np[i])\n",
        "\n",
        "    current_len = len(filtered_embeddings_list)\n",
        "\n",
        "    if current_len == 0:\n",
        "        return np.full((max_sequence_length_for_lstm, embedding_dim_bert), padding_zeroes, dtype=np.float32)\n",
        "\n",
        "    # Initialize the final embeddings array with padding zeroes\n",
        "    final_embeddings_array = np.full((max_sequence_length_for_lstm, embedding_dim_bert), padding_zeroes, dtype=np.float32)\n",
        "\n",
        "    # If the current length is greater than or equal to the max sequence length (50), truncate the list\n",
        "    # else, fill the array with the available embeddings\n",
        "    if current_len >= max_sequence_length_for_lstm:\n",
        "        final_embeddings_array = np.array(filtered_embeddings_list[:max_sequence_length_for_lstm], dtype=np.float32)\n",
        "    else:\n",
        "        final_embeddings_array[:current_len] = np.array(filtered_embeddings_list, dtype=np.float32)\n",
        "\n",
        "    return final_embeddings_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4b0f8a00",
      "metadata": {
        "id": "4b0f8a00"
      },
      "outputs": [],
      "source": [
        "# Getting bert embeddings for each word in the sentence\n",
        "list_of_embeddings_BERT = [\n",
        "   get_bert_embeddings(text, bert_model, bert_tokenizer, max_sequence_length)\n",
        "    for text in ISEAR_data['text']\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8a6e350c",
      "metadata": {
        "id": "8a6e350c"
      },
      "outputs": [],
      "source": [
        "# Padding sequences to the same length -> adding zeroes to the sequences shorter than embedding_dim and cutting sequences longer than embedding_dim\n",
        "# Converting to NumPy array\n",
        "embeddings_BERT = pad_sequences(\n",
        "    list_of_embeddings_BERT,\n",
        "    maxlen = max_sequence_length,\n",
        "    dtype = 'float32',\n",
        "    padding = 'post',\n",
        "    truncating = 'post'\n",
        ")\n",
        "\n",
        "# print (embeddings_BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "de4ea499",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de4ea499",
        "outputId": "2487fdcc-4c8a-4c76-e6e7-98f1991c12ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensions of embeddings_BERT: (7505, 30, 768)\n",
            "Dimensions of target value: (7505, 7)\n"
          ]
        }
      ],
      "source": [
        "# One-hot encoding of target values\n",
        "y = to_categorical(ISEAR_data['emotion_label'], num_classes=num_classes)\n",
        "\n",
        "print(f\"\\nDimensions of embeddings_BERT: {embeddings_BERT.shape}\") # (number_of_examples, sequence_length, embedding_dim)\n",
        "print(f\"Dimensions of target value: {y.shape}\") # (number_of_examples, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc2d0dc",
      "metadata": {
        "id": "1dc2d0dc"
      },
      "source": [
        "# RNN predictions for BERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a5f360f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5f360f6",
        "outputId": "ce4bd053-8cfd-4dec-ddfa-108d33a4b43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Fold 1/5 ---\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "   Fold 1 - Val Loss: 1.2539, Val Acc: 0.6203, Val Precision-macro: 0.6190, Val Recall-macro: 0.6204, Val F1-macro: 0.6170, Val AUC-macro: 0.8934\n",
            "\n",
            "--- Training Fold 2/5 ---\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "   Fold 2 - Val Loss: 1.1440, Val Acc: 0.6129, Val Precision-macro: 0.6184, Val Recall-macro: 0.6127, Val F1-macro: 0.6098, Val AUC-macro: 0.8922\n",
            "\n",
            "--- Training Fold 3/5 ---\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "   Fold 3 - Val Loss: 1.0828, Val Acc: 0.6482, Val Precision-macro: 0.6478, Val Recall-macro: 0.6486, Val F1-macro: 0.6453, Val AUC-macro: 0.9054\n",
            "\n",
            "--- Training Fold 4/5 ---\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "   Fold 4 - Val Loss: 1.1946, Val Acc: 0.6243, Val Precision-macro: 0.6234, Val Recall-macro: 0.6246, Val F1-macro: 0.6221, Val AUC-macro: 0.8954\n",
            "\n",
            "--- Training Fold 5/5 ---\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "   Fold 5 - Val Loss: 1.1202, Val Acc: 0.6189, Val Precision-macro: 0.6280, Val Recall-macro: 0.6192, Val F1-macro: 0.6191, Val AUC-macro: 0.8998\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "n_splits = 5 # 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "lstm_units = 128\n",
        "dropout_rate = 0.3\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_losses = []\n",
        "fold_f1_macros = []\n",
        "fold_auc_macros = []\n",
        "fold_precision_macros = []\n",
        "fold_recall_macros = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(embeddings_BERT, y_original_labels)):\n",
        "    print(f\"\\n--- Training Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    X_train_fold, X_val_fold = embeddings_BERT[train_index], embeddings_BERT[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    clear_session() # Reseting Keras session to build new model\n",
        "\n",
        "    # Building an LSTM Bidirectional model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(max_sequence_length, embedding_dim_bert)))\n",
        "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=False)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # EarlyStopping Callback - to prevent model of overfitting\n",
        "    early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_accuracy', # We are tracking validation accuracy\n",
        "        patience=5,             # The training stops if the val_accuracy doesn't get better after 5 epochs\n",
        "        restore_best_weights=True,\n",
        "        mode='max',             # Maximizing accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Training the model\n",
        "    history = model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        epochs=50,   # trining in 50 epochs (we assume early stopping call back will end the training before)\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        callbacks=[early_stopping_callback],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluating the model\n",
        "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "    fold_losses.append(loss)\n",
        "    fold_accuracies.append(accuracy)\n",
        "\n",
        "    y_pred_probs_fold = model.predict(X_val_fold, verbose=0)\n",
        "    y_pred_classes_fold = np.argmax(y_pred_probs_fold, axis=1)\n",
        "    y_true_classes_fold = np.argmax(y_val_fold, axis=1)\n",
        "\n",
        "    precision_macro_fold = precision_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "    recall_macro_fold = recall_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "\n",
        "    fold_precision_macros.append(precision_macro_fold)\n",
        "    fold_recall_macros.append(recall_macro_fold)\n",
        "\n",
        "    f1_macro_fold = f1_score(y_true_classes_fold, y_pred_classes_fold, average='macro', zero_division=0)\n",
        "    fold_f1_macros.append(f1_macro_fold)\n",
        "\n",
        "    try:\n",
        "        auc_macro_fold = roc_auc_score(y_true_classes_fold, y_pred_probs_fold, multi_class='ovr', average='macro')\n",
        "        fold_auc_macros.append(auc_macro_fold)\n",
        "    except ValueError as e:\n",
        "        fold_auc_macros.append(np.nan)\n",
        "\n",
        "    # Printing results for the current fold\n",
        "    print(f\"   Fold {fold + 1} - Val Loss: {loss:.4f}, Val Acc: {accuracy:.4f}, Val Precision-macro: {precision_macro_fold:.4f}, Val Recall-macro: {recall_macro_fold:.4f}, Val F1-macro: {f1_macro_fold:.4f}, Val AUC-macro: {fold_auc_macros[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "33ebdc48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ebdc48",
        "outputId": "d213f497-cc92-4a4e-d4d9-085a68acb7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Average cross validation results for Word2Vec BERT embeddings ---\n",
            "Val Loss: 1.1591 +/- 0.0597\n",
            "Val Accuracy: 0.6249 +/- 0.0122\n",
            "Val Precision-macro: 0.6273 +/- 0.0108\n",
            "Val Recall-macro: 0.6251 +/- 0.0124\n",
            "Val F1-macro: 0.6227 +/- 0.0120\n",
            "Val AUC-macro: 0.8972 +/- 0.0048\n"
          ]
        }
      ],
      "source": [
        "# Calculation of average results across all folds +/ std.dev\n",
        "print(\"\\n--- Average cross validation results for Word2Vec BERT embeddings ---\")\n",
        "print(f\"Val Loss: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\")\n",
        "print(f\"Val Accuracy: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Val Precision-macro: {np.mean(fold_precision_macros):.4f} +/- {np.std(fold_precision_macros):.4f}\")\n",
        "print(f\"Val Recall-macro: {np.mean(fold_recall_macros):.4f} +/- {np.std(fold_recall_macros):.4f}\")\n",
        "print(f\"Val F1-macro: {np.mean(fold_f1_macros):.4f} +/- {np.std(fold_f1_macros):.4f}\")\n",
        "print(f\"Val AUC-macro: {np.nanmean(fold_auc_macros):.4f} +/- {np.nanstd(fold_auc_macros):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}