{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nina DobÅ¡a, zadnje ureÄ‘ivano 7.7.2025.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from datasets import load_dataset\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import - NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of data from local file\n",
    "emotion_lexicon = pd.read_excel(\"data/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx\", sheet_name=\"NRC-Lex-v0.92-word-translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14182, 115)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing number of rows and columns in emotion lexicon\n",
    "emotion_lexicon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['English (en)', 'Afrikaans (af)', 'Albanian (sq)', 'Amharic (am)',\n",
       "       'Arabic (ar)', 'Armenian (hy)', 'Azeerbaijani (az)', 'Basque (eu)',\n",
       "       'Belarusian (be)', 'Bengali (bn)',\n",
       "       ...\n",
       "       'Positive', 'Negative', 'Anger', 'Anticipation', 'Disgust', 'Fear',\n",
       "       'Joy', 'Sadness', 'Surprise', 'Trust'],\n",
       "      dtype='object', length=115)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing columns from the dataset\n",
    "emotion_lexicon.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of fine tuned word2vec model\n",
    "model_path_SG = \"fine_tuned_word2vec_sg/fine_tuned_word2vec_sg.model\"\n",
    "model_path_CBOW = \"fine_tuned_word2vec_cbow/fine_tuned_word2vec_cbow.model\"\n",
    "word2vec_model_SG = Word2Vec.load(model_path_SG)\n",
    "word2vec_model_CBOW = Word2Vec.load(model_path_CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the coverage of words from emotion lexicon by word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon coverage: 87.17%\n",
      "Covered words: 12362\n"
     ]
    }
   ],
   "source": [
    "lexicon_words = set(emotion_lexicon[\"English (en)\"])\n",
    "word2vec_vocab_SG = set(word2vec_model_SG.wv.key_to_index.keys())\n",
    "\n",
    "covered_words = lexicon_words & word2vec_vocab_SG # Intersection of lexicon words and word2vec vocabulary\n",
    "coverage_percentage = len(covered_words) / len(lexicon_words) * 100\n",
    "\n",
    "print(f\"Lexicon coverage: {coverage_percentage:.2f}%\")\n",
    "print(f\"Covered words: {len(covered_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon coverage: 87.17%\n",
      "Covered words: 12362\n"
     ]
    }
   ],
   "source": [
    "word2vec_vocab_CBOW = set(word2vec_model_CBOW.wv.key_to_index.keys())\n",
    "\n",
    "covered_words = lexicon_words & word2vec_vocab_CBOW # Intersection of lexicon words and word2vec vocabulary\n",
    "coverage_percentage = len(covered_words) / len(lexicon_words) * 100\n",
    "\n",
    "print(f\"Lexicon coverage: {coverage_percentage:.2f}%\")\n",
    "print(f\"Covered words: {len(covered_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covered words: 12362\n"
     ]
    }
   ],
   "source": [
    "intersection = word2vec_vocab_SG & word2vec_vocab_CBOW & lexicon_words # Intersection of lexicon words and word2vec vocabulary\n",
    "\n",
    "print(f\"Covered words: {len(intersection)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering only covered words and columns; word, anger, joy, sadness, disgust, surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English (en)</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English (en)  Anger  Joy  Sadness  Disgust  Surprise\n",
       "0        aback      0    0        0        0         0\n",
       "1      abandon      0    0        1        0         0\n",
       "2    abandoned      1    0        1        0         0\n",
       "3  abandonment      1    0        1        0         1\n",
       "4        abate      0    0        0        0         0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lexicon = emotion_lexicon[emotion_lexicon[\"English (en)\"].isin(word2vec_vocab_SG)].reset_index(drop=True)\n",
    "filtered_lexicon = filtered_lexicon[[\"English (en)\", \"Anger\", \"Joy\", \"Sadness\", \"Disgust\", \"Surprise\"]]\n",
    "filtered_lexicon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting word2vec embedding for an input word from lexicon\n",
    "def get_word2vec_embedding(word, model):\n",
    "    try:\n",
    "        return model.wv[word] # Return the embedding for the word\n",
    "    except KeyError:\n",
    "        return np.zeros(model.vector_size) # Return a zero-vector if the word is not in the vocabulary (this should never be the case since we filtered lexicon already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English (en)</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>word2vec_embedding_SG</th>\n",
       "      <th>word2vec_embedding_CBOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.05127184, 0.09056614, 0.18572424, 0.125842...</td>\n",
       "      <td>[0.042214748, -0.034687907, 0.15201779, 0.1485...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.099102534, 0.19064683, -0.14152387, 0.0222...</td>\n",
       "      <td>[-0.046076052, 0.367438, -0.17085469, 0.096703...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.015122078, 0.4042172, -0.009172466, -0.211...</td>\n",
       "      <td>[-0.13989627, 0.41552946, 0.1068828, -0.094600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.13099883, 0.18350211, -0.023655038, 0.1231...</td>\n",
       "      <td>[-0.118318245, 0.30210206, 0.10299667, 0.23302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.028459722, 0.075622424, 0.0515427, 0.031762...</td>\n",
       "      <td>[0.014462058, 0.024446249, 0.037190884, 0.0148...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English (en)  Anger  Joy  Sadness  Disgust  Surprise  \\\n",
       "0        aback      0    0        0        0         0   \n",
       "1      abandon      0    0        1        0         0   \n",
       "2    abandoned      1    0        1        0         0   \n",
       "3  abandonment      1    0        1        0         1   \n",
       "4        abate      0    0        0        0         0   \n",
       "\n",
       "                               word2vec_embedding_SG  \\\n",
       "0  [-0.05127184, 0.09056614, 0.18572424, 0.125842...   \n",
       "1  [-0.099102534, 0.19064683, -0.14152387, 0.0222...   \n",
       "2  [-0.015122078, 0.4042172, -0.009172466, -0.211...   \n",
       "3  [-0.13099883, 0.18350211, -0.023655038, 0.1231...   \n",
       "4  [0.028459722, 0.075622424, 0.0515427, 0.031762...   \n",
       "\n",
       "                             word2vec_embedding_CBOW  \n",
       "0  [0.042214748, -0.034687907, 0.15201779, 0.1485...  \n",
       "1  [-0.046076052, 0.367438, -0.17085469, 0.096703...  \n",
       "2  [-0.13989627, 0.41552946, 0.1068828, -0.094600...  \n",
       "3  [-0.118318245, 0.30210206, 0.10299667, 0.23302...  \n",
       "4  [0.014462058, 0.024446249, 0.037190884, 0.0148...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying get_word2vec_embedding function to every word in filtered dictionary\n",
    "filtered_lexicon['word2vec_embedding_SG'] = filtered_lexicon['English (en)'].apply(\n",
    "    lambda word: get_word2vec_embedding(word, word2vec_model_SG)\n",
    ")\n",
    "\n",
    "filtered_lexicon['word2vec_embedding_CBOW'] = filtered_lexicon['English (en)'].apply(\n",
    "    lambda word: get_word2vec_embedding(word, word2vec_model_CBOW)\n",
    ")\n",
    "\n",
    "filtered_lexicon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm for Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGNS embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_SG = np.array(filtered_lexicon['word2vec_embedding_SG'].tolist())  # Word embeddings as features\n",
    "y_anger = filtered_lexicon['Anger']  \n",
    "y_joy = filtered_lexicon['Joy']\n",
    "y_sadness = filtered_lexicon['Sadness']\n",
    "y_disgust = filtered_lexicon['Disgust']\n",
    "y_surprise = filtered_lexicon['Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Performing 5-fold cross validation\n",
    "scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "scores_anger_sg = cross_validate(clf, X_SG, y_anger, cv=kf, scoring=scoring_metrics)\n",
    "scores_joy_sg = cross_validate(clf, X_SG, y_joy, cv=kf, scoring=scoring_metrics)\n",
    "scores_sadness_sg = cross_validate(clf, X_SG, y_sadness, cv=kf, scoring=scoring_metrics)\n",
    "scores_disgust_sg = cross_validate(clf, X_SG, y_disgust, cv=kf, scoring=scoring_metrics)\n",
    "scores_surprise_sg = cross_validate(clf, X_SG, y_surprise, cv=kf, scoring=scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy anger: 0.9079 +/- 0.0006\n",
      "accuracy joy: 0.9467 +/- 0.0005\n",
      "accuracy sadness: 0.9123 +/- 0.0007\n",
      "accuracy disgust: 0.9216 +/- 0.0005\n",
      "accuracy surprise: 0.9589 +/- 0.0002\n",
      "\n",
      "\n",
      "precision_macro anger: 0.7225 +/- 0.1724\n",
      "precision_macro joy: 0.8234 +/- 0.2001\n",
      "precision_macro sadness: 0.8527 +/- 0.0949\n",
      "precision_macro disgust: 0.8359 +/- 0.1938\n",
      "precision_macro surprise: 0.5795 +/- 0.2001\n",
      "\n",
      "\n",
      "recall_macro anger: 0.5037 +/- 0.0026\n",
      "recall_macro joy: 0.5052 +/- 0.0039\n",
      "recall_macro sadness: 0.5088 +/- 0.0021\n",
      "recall_macro disgust: 0.5040 +/- 0.0026\n",
      "recall_macro surprise: 0.5010 +/- 0.0020\n",
      "\n",
      "\n",
      "f1_macro anger: 0.4836 +/- 0.0051\n",
      "f1_macro joy: 0.4967 +/- 0.0076\n",
      "f1_macro sadness: 0.4948 +/- 0.0040\n",
      "f1_macro disgust: 0.4877 +/- 0.0052\n",
      "f1_macro surprise: 0.4915 +/- 0.0039\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating and printing the avg score +/- std dev\n",
    "for metric in scoring_metrics:\n",
    "    avg_score_anger_sg = np.mean(scores_anger_sg[f'test_{metric}'])\n",
    "    std_dev_anger_sg = np.std(scores_anger_sg[f'test_{metric}'])\n",
    "\n",
    "    avg_score_joy_sg = np.mean(scores_joy_sg[f'test_{metric}'])\n",
    "    std_dev_joy_sg = np.std(scores_joy_sg[f'test_{metric}'])\n",
    "\n",
    "    avg_score_sadness_sg = np.mean(scores_sadness_sg[f'test_{metric}'])\n",
    "    std_dev_sadness_sg = np.std(scores_sadness_sg[f'test_{metric}'])\n",
    "\n",
    "    avg_score_disgust_sg = np.mean(scores_disgust_sg[f'test_{metric}'])\n",
    "    std_dev_disgust_sg = np.std(scores_disgust_sg[f'test_{metric}'])\n",
    "\n",
    "    avg_score_surprise_sg = np.mean(scores_surprise_sg[f'test_{metric}'])\n",
    "    std_dev_surprise_sg = np.std(scores_surprise_sg[f'test_{metric}'])\n",
    "\n",
    "\n",
    "    print(f\"{metric} anger: {avg_score_anger_sg:.4f} +/- {std_dev_anger_sg:.4f}\")\n",
    "    print(f\"{metric} joy: {avg_score_joy_sg:.4f} +/- {std_dev_joy_sg:.4f}\")\n",
    "    print(f\"{metric} sadness: {avg_score_sadness_sg:.4f} +/- {std_dev_sadness_sg:.4f}\")\n",
    "    print(f\"{metric} disgust: {avg_score_disgust_sg:.4f} +/- {std_dev_disgust_sg:.4f}\")\n",
    "    print(f\"{metric} surprise: {avg_score_surprise_sg:.4f} +/- {std_dev_surprise_sg:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_CBOW = np.array(filtered_lexicon['word2vec_embedding_CBOW'].tolist())  # Word embeddings as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Performing 5-fold cross validation\n",
    "scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "scores_anger_cbow = cross_validate(clf, X_CBOW, y_anger, cv=kf, scoring=scoring_metrics)\n",
    "scores_joy_cbow = cross_validate(clf, X_CBOW, y_joy, cv=kf, scoring=scoring_metrics)\n",
    "scores_sadness_cbow = cross_validate(clf, X_CBOW, y_sadness, cv=kf, scoring=scoring_metrics)\n",
    "scores_disgust_cbow = cross_validate(clf, X_CBOW, y_disgust, cv=kf, scoring=scoring_metrics)\n",
    "scores_surprise_cbow = cross_validate(clf, X_CBOW, y_surprise, cv=kf, scoring=scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy anger: 0.9083 +/- 0.0005\n",
      "accuracy joy: 0.9474 +/- 0.0007\n",
      "accuracy sadness: 0.9121 +/- 0.0011\n",
      "accuracy disgust: 0.9214 +/- 0.0004\n",
      "accuracy surprise: 0.9587 +/- 0.0004\n",
      "\n",
      "\n",
      "precision_macro anger: 0.8193 +/- 0.0862\n",
      "precision_macro joy: 0.8506 +/- 0.0729\n",
      "precision_macro sadness: 0.7698 +/- 0.0856\n",
      "precision_macro disgust: 0.7774 +/- 0.1857\n",
      "precision_macro surprise: 0.4794 +/- 0.0001\n",
      "\n",
      "\n",
      "recall_macro anger: 0.5059 +/- 0.0021\n",
      "recall_macro joy: 0.5148 +/- 0.0054\n",
      "recall_macro sadness: 0.5112 +/- 0.0049\n",
      "recall_macro disgust: 0.5029 +/- 0.0020\n",
      "recall_macro surprise: 0.4999 +/- 0.0002\n",
      "\n",
      "\n",
      "f1_macro anger: 0.4880 +/- 0.0042\n",
      "f1_macro joy: 0.5154 +/- 0.0102\n",
      "f1_macro sadness: 0.4997 +/- 0.0094\n",
      "f1_macro disgust: 0.4856 +/- 0.0039\n",
      "f1_macro surprise: 0.4894 +/- 0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating and printing the avg score +/- std dev\n",
    "for metric in scoring_metrics:\n",
    "    avg_score_anger_cbow = np.mean(scores_anger_cbow[f'test_{metric}'])\n",
    "    std_dev_anger_cbow = np.std(scores_anger_cbow[f'test_{metric}'])\n",
    "\n",
    "    avg_score_joy_cbow = np.mean(scores_joy_cbow[f'test_{metric}'])\n",
    "    std_dev_joy_cbow = np.std(scores_joy_cbow[f'test_{metric}'])\n",
    "\n",
    "    avg_score_sadness_cbow = np.mean(scores_sadness_cbow[f'test_{metric}'])\n",
    "    std_dev_sadness_cbow = np.std(scores_sadness_cbow[f'test_{metric}'])\n",
    "\n",
    "    avg_score_disgust_cbow = np.mean(scores_disgust_cbow[f'test_{metric}'])\n",
    "    std_dev_disgust_cbow = np.std(scores_disgust_cbow[f'test_{metric}'])\n",
    "\n",
    "    avg_score_surprise_cbow = np.mean(scores_surprise_cbow[f'test_{metric}'])\n",
    "    std_dev_surprise_cbow = np.std(scores_surprise_cbow[f'test_{metric}'])\n",
    "\n",
    "\n",
    "    print(f\"{metric} anger: {avg_score_anger_cbow:.4f} +/- {std_dev_anger_cbow:.4f}\")\n",
    "    print(f\"{metric} joy: {avg_score_joy_cbow:.4f} +/- {std_dev_joy_cbow:.4f}\")\n",
    "    print(f\"{metric} sadness: {avg_score_sadness_cbow:.4f} +/- {std_dev_sadness_cbow:.4f}\")\n",
    "    print(f\"{metric} disgust: {avg_score_disgust_cbow:.4f} +/- {std_dev_disgust_cbow:.4f}\")\n",
    "    print(f\"{metric} surprise: {avg_score_surprise_cbow:.4f} +/- {std_dev_surprise_cbow:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Import of fine tuned bert model\n",
    "model_path = \"fine_tuned_bert\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "bert_model = BertForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting a BERT embedding for an input word from lexicon\n",
    "def get_bert_embedding(word, model, tokenizer):\n",
    "\n",
    "    # Tokenize word and convert to tensor\n",
    "    input_ids = tokenizer.encode(word, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        outputs = model.bert(input_ids)  # Extract only transformer layers, ignoring MLM head\n",
    "        last_hidden_state = outputs.last_hidden_state  # Get hidden state of the last layer\n",
    "\n",
    "    # Calculate the average embedding across tokens\n",
    "    word_embedding = last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting BERT embeddings\n",
    "bert_model.eval()\n",
    "filtered_lexicon['bert_embedding'] = filtered_lexicon['English (en)'].apply(\n",
    "    lambda word: get_bert_embedding(word, bert_model, bert_tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm for BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_BERT = np.array(filtered_lexicon['bert_embedding'].tolist())  # Word embeddings as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Nina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Performing 5-fold cross validation\n",
    "scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "scores_anger_bert = cross_validate(clf, X_BERT, y_anger, cv=kf, scoring=scoring_metrics)\n",
    "scores_joy_bert = cross_validate(clf, X_BERT, y_joy, cv=kf, scoring=scoring_metrics)\n",
    "scores_sadness_bert = cross_validate(clf, X_BERT, y_sadness, cv=kf, scoring=scoring_metrics)\n",
    "scores_disgust_bert = cross_validate(clf, X_BERT, y_disgust, cv=kf, scoring=scoring_metrics)\n",
    "scores_surprise_bert = cross_validate(clf, X_BERT, y_surprise, cv=kf, scoring=scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy anger: 0.9075 +/- 0.0005\n",
      "accuracy joy: 0.9463 +/- 0.0002\n",
      "accuracy sadness: 0.9113 +/- 0.0003\n",
      "accuracy disgust: 0.9211 +/- 0.0000\n",
      "accuracy surprise: 0.9588 +/- 0.0002\n",
      "\n",
      "\n",
      "precision_macro anger: 0.4538 +/- 0.0001\n",
      "precision_macro joy: 0.4731 +/- 0.0001\n",
      "precision_macro sadness: 0.5556 +/- 0.2001\n",
      "precision_macro disgust: 0.4606 +/- 0.0000\n",
      "precision_macro surprise: 0.4794 +/- 0.0001\n",
      "\n",
      "\n",
      "recall_macro anger: 0.4999 +/- 0.0002\n",
      "recall_macro joy: 0.5000 +/- 0.0000\n",
      "recall_macro sadness: 0.5005 +/- 0.0009\n",
      "recall_macro disgust: 0.5000 +/- 0.0000\n",
      "recall_macro surprise: 0.5000 +/- 0.0000\n",
      "\n",
      "\n",
      "f1_macro anger: 0.4757 +/- 0.0001\n",
      "f1_macro joy: 0.4862 +/- 0.0000\n",
      "f1_macro sadness: 0.4777 +/- 0.0019\n",
      "f1_macro disgust: 0.4795 +/- 0.0000\n",
      "f1_macro surprise: 0.4895 +/- 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating and printing the avg score +/- std dev\n",
    "for metric in scoring_metrics:\n",
    "    avg_score_anger_bert = np.mean(scores_anger_bert[f'test_{metric}'])\n",
    "    std_dev_anger_bert = np.std(scores_anger_bert[f'test_{metric}'])\n",
    "\n",
    "    avg_score_joy_bert = np.mean(scores_joy_bert[f'test_{metric}'])\n",
    "    std_dev_joy_bert = np.std(scores_joy_bert[f'test_{metric}'])\n",
    "\n",
    "    avg_score_sadness_bert = np.mean(scores_sadness_bert[f'test_{metric}'])\n",
    "    std_dev_sadness_bert = np.std(scores_sadness_bert[f'test_{metric}'])\n",
    "\n",
    "    avg_score_disgust_bert = np.mean(scores_disgust_bert[f'test_{metric}'])\n",
    "    std_dev_disgust_bert = np.std(scores_disgust_bert[f'test_{metric}'])\n",
    "\n",
    "    avg_score_surprise_bert = np.mean(scores_surprise_bert[f'test_{metric}'])\n",
    "    std_dev_surprise_bert = np.std(scores_surprise_bert[f'test_{metric}'])\n",
    "\n",
    "\n",
    "    print(f\"{metric} anger: {avg_score_anger_bert:.4f} +/- {std_dev_anger_bert:.4f}\")\n",
    "    print(f\"{metric} joy: {avg_score_joy_bert:.4f} +/- {std_dev_joy_bert:.4f}\")\n",
    "    print(f\"{metric} sadness: {avg_score_sadness_bert:.4f} +/- {std_dev_sadness_bert:.4f}\")\n",
    "    print(f\"{metric} disgust: {avg_score_disgust_bert:.4f} +/- {std_dev_disgust_bert:.4f}\")\n",
    "    print(f\"{metric} surprise: {avg_score_surprise_bert:.4f} +/- {std_dev_surprise_bert:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
